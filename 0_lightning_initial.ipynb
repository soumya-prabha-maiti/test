{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Pytorch lightning demo (MNIST digit classification)\n",
    "\n",
    "Dataset: MNIST\n",
    "\n",
    "Model: Feedforward NN\n",
    "\n",
    "Libraries: Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import random_split\n",
    "import lightning as L\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class MyAccuracy(Metric):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "    \n",
    "    def update(self, preds, target):\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        assert preds.shape == target.shape\n",
    "        self.correct += torch.sum(preds == target)\n",
    "        self.total += target.numel()\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.correct.float() / self.total.float()\n",
    "\n",
    "    \n",
    "class NNlightning(L.LightningModule):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.myaccuracy = MyAccuracy()\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx, step_type: str):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_function(y_hat, y)\n",
    "        self.log(f'{step_type}_loss', loss)\n",
    "        return loss, y_hat, y\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._common_step(batch, batch_idx, \"train\")\n",
    "        accuracy = self.accuracy(y_hat, y)\n",
    "        f1_score = self.f1_score(y_hat, y)\n",
    "        myaccuracy = self.myaccuracy(y_hat, y)\n",
    "        self.log_dict({'train_accuracy': accuracy, 'train_f1_score': f1_score, \"train_myaccuracy\": myaccuracy}, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._common_step(batch, batch_idx, \"val\")\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._common_step(batch, batch_idx, \"test\")\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._common_step(batch, batch_idx, \"test\")\n",
    "        return torch.argmax(y_hat)\n",
    "    \n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        return super().on_train_epoch_end()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "# class MNISTDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, data_dir: str = \"/path/to/data\", batch_size: int = 64):\n",
    "#         super().__init__()\n",
    "#         self.data_dir = data_dir\n",
    "#         self.batch_size = batch_size\n",
    "#         self.transform = transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,), (0.5,))\n",
    "#         ])\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n",
    "#         MNIST(self.data_dir, train=False, download=True, transform=self.transform)\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         self.mnist_train = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "#         self.mnist_val = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.mnist_train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.mnist_val, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "entire_dataset = datasets.MNIST(root=\"./MNIST_data\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_ds, val_ds = random_split(entire_dataset, [50000, 10000])\n",
    "test_ds = datasets.MNIST(root=\"./MNIST_data\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "model = NNlightning(input_size=input_size, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Train Network\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "#         # Get data to cuda if possible\n",
    "#         data = data.to(device=device)\n",
    "#         targets = targets.to(device=device)\n",
    "\n",
    "#         # Get to correct shape\n",
    "#         data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "#         # Forward\n",
    "#         scores = model(data)\n",
    "#         loss = criterion(scores, targets)\n",
    "\n",
    "#         # Backward\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Gradient descent or adam step\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/soumyapm/Scratchpad/env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name          | Type               | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | fc1           | Linear             | 39.2 K | train\n",
      "1 | fc2           | Linear             | 510    | train\n",
      "2 | loss_function | CrossEntropyLoss   | 0      | train\n",
      "3 | accuracy      | MulticlassAccuracy | 0      | train\n",
      "4 | myaccuracy    | MyAccuracy         | 0      | train\n",
      "5 | f1_score      | MulticlassF1Score  | 0      | train\n",
      "-------------------------------------------------------------\n",
      "39.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "39.8 K    Total params\n",
      "0.159     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soumyapm/Scratchpad/env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/Users/soumyapm/Scratchpad/env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/soumyapm/Scratchpad/env/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 782/782 [00:12<00:00, 64.19it/s, v_num=13, train_accuracy_step=1.000, train_f1_score_step=1.000, train_myaccuracy_step=1.000, train_accuracy_epoch=0.883, train_f1_score_epoch=0.883, train_myaccuracy_epoch=0.883]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 782/782 [00:12<00:00, 64.12it/s, v_num=13, train_accuracy_step=1.000, train_f1_score_step=1.000, train_myaccuracy_step=1.000, train_accuracy_epoch=0.883, train_f1_score_epoch=0.883, train_myaccuracy_epoch=0.883]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=1)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Get to correct shape\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "model.to(device)\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on validation set: {check_accuracy(val_loader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
